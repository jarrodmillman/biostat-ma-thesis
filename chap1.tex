\chapter{\label{ch:intro}Introduction}

\texttt{permute} is a Python package for permutation tests and confidence
sets.\footnote{\url{http://statlab.github.io/permute}}
Philip B. Stark, Kellie Ottoboni, and I developed this package over the
last year with most of the work occurring during the last few months.
In this report, I briefly explain the purpose of the package (\S~\ref{ch:intro}), our
development practices (\S~\ref{ch:dev}), the currently available functionality (\S~\ref{ch:func}), and
our immediate and long-term roadmap (\S~\ref{ch:nextsteps}).

\section{Permutation tests and confidence sets}

Permutation tests (sometimes referred to as randomization, re-randomization, or
exact tests) are a nonparametric approach to statistical significance
testing.  In a permutation test, the distribution of the test statistic under
the null hypothesis is obtained (exactly or approximately) by considering or
computing the test statistics of all possible relabelings of the observed data.

For example, imagine you observe 10 coin tosses (e.g., HTTHTHTTTT). Assume 
each trial is independent and identically distributed and let the
test statistic be the number of Hs. Under the null hypothesis that the
coin is unbiased, the Hs and Ts are exchangeable and the expected value of the
test statistic is 5. To find the distribution of this test statistic under the
null, count all possible sequences yielding a test statistic of 0,
1, 2, ..., or 10 Hs in 10 tosses.

Consider another example from regression.  In this case, the observed data are
10 $(x, y)$ pairs, which we wish to fit (using least squares) to a linear model
$y = a + bx + \epsilon$.  To test whether $b = 0$, let's use
$\hat{b}/SE(\hat{b})$ as the test statistic.  Under the null hypothesis, there
is no linear relation between $x$ and $y$. Hence the distribution of the test
statistic is found by computing it on all possible $(x, y)$ formed by permuting
the $y$ values among the $x$ values.

From the distribution of the test statistic under the null, you compute
p-values by taking the ratio of the count of the \emph{as extreme} or
\emph{more extreme} test statistics to the total number of such test
statistics.  For instance, in the first example, the test statistic is 3.
Under the null, the expected test statistic is 5.  Thus the more extreme test
statistics are 0, 1, 2, 3, 8, 9, and 10. So the p-value is
\begin{align*}
\frac{\binom{10}{0} + \binom{10}{1} + \binom{10}{2} + \binom{10}{3} +
   \binom{10}{8} + \binom{10}{9} + \binom{10}{10}}{2^{10}}.
\end{align*}
In the second example, there is no analytic expression; however, the
computational procedure is straightforward.

To summarize, the general procedure for computing a p-value using permutation
testing is:
\begin{enumerate}
\item carefully formulate the null hypothesis
\item from the observed data generate all equally likely possible datasets
\item compute test statistic for the observed and generated data
\item calculate ratio of data sets with at least as extreme test statistic over the total number of such sets.
\end{enumerate}
As the above examples suggest, this procedure may be computationally
intractable in practice.  In this case, simulation can often produce
good approximate results.

The advantage of this approach is that it produces exact (or approximately
exact) results using stronger null hypotheses and weaker assumptions about
the generating process. Additionally, the rationale justifying these claims
doesn't require asymptotic theory.  This means it should be easier for
non-mathematically trained scientists to fully understand.  However, the
obvious disadvantage is the computational cost.

What does permute aim to provide

...


\section{Python}

Python is a high-level, general purpose programming language, which has become
increasingly popular for scientific computing \cite{millman2011python,
Perez2011}. Unlike some high-level languages used in scientific computing,
Python was not specifically designed for scientific applications.  However, it
quickly attracted interest among scientists and engineers.  Initially, it was
employed primarily as a ``glue'' language to couple together low-level numeric
libraries written in C or Fortran with higher-level scientific application
languages such as Matlab \cite{dubois2007guest}.

As more scientists and engineers began using Python, they started developing
third party libraries to provide additional functionality for scientific
and numeric computing.  In particular, NumPy\footnote{\url{http://numpy.org}},
SciPy\footnote{\url{http://scipy.org}}, and matplotlib\footnote{
\url{http://matplotlib.org}} provide a core foundation on which other
scientific Python packages (such as \texttt{permute}) build. NumPy
provides the basic n-dimensional array data structure and a small core
functionality such as linear algebra routines to compute on this
data structure.  SciPy adds several additional general routines on top
of this core functionality necessary for scientific computing including
basic statistics and optimization.  Complementing these data structures
and procedures, matplotlib provides publication quality 2D plotting.  

While it is beyond the scope of this report to explore these packages in more
detail, I note that the pseudo-random number generator (PRNG) provide by NumPy
is the Mersenne Twister.  The Mersenne Twister is an efficient PRNG with a
sufficiently large period for most statistical simulations.\footnote{It is
default PRNG in R as well.  While sufficient for most statistical simulations,
for other applications such as cryptography it may be insufficient.}  In
addition to providing a high-quality PRNG, NumPy implements Knuth shuffling ---
an efficient (and simple) algorithm for uniformly generating permutations of
sequences.

While Python is similar to R in many respects and is widely used in scientific
and numerical computing, it lacks R extensive support for statistical
applications.  Recently, however, it has become more attractive for statistical
applications due to several new packages such as Pandas, statsmodels, and
scikit-learn.  Our intention is to help accelerate this trend by supplying a
high-quality, rigorously tested, and statistically sound package for a large
variety of permutation tests and confidence sets. As the package matures, we
anticipate contributing generic functionality upstream to the packages we
depend on as appropriate.

missing high-quality resampling approaches

we might consider adding bootstrapping like ...
