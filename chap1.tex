\chapter{\label{ch:intro}Introduction}

\texttt{permute} is a Python package for permutation tests and confidence
sets.\footnote{\url{http://statlab.github.io/permute}} Philip B. Stark, Kellie
Ottoboni, St\'{e}fan van der Walt, and I developed this package over the last
year with most of the work occurring during the last few months.  In this
report, I briefly explain the purpose of the package (\S~\ref{ch:intro}), our
development practices (\S~\ref{ch:dev}), the current functionality
(\S~\ref{ch:func}), and our short- and long-term plans (\S~\ref{ch:nextsteps}).

\section{Permutation tests}

Permutation tests (sometimes referred to as randomization, re-randomization, or
exact tests) are a nonparametric approach to statistical significance testing.
They were first introduced by R. A. Fisher in 1935 \cite{fisher1935design} and
further developed by E. J. G. Pitman  \cite{pitman1937,
pitman1938significance}.  After the introduction of the bootstrap, the ideas
were extended in the late 1980's by J. Romano \cite{romano1988bootstrap,
romano1989bootstrap}.
Abstract permutation tests are possible whenever the
null distribution of the data is invariant under the action of some group.\footnote{A group $\mathcal{G} = (G, \cdot)$ is a tuple
where $G$ is a set of elements and $\cdot$ is an operation that takes any two elements
of $G$ and returns another element of $G$ such that (1) $G$ is \emph{closed} under $\cdot$,
(2) $\cdot$ is \emph{associative}, (3) there exist an $e \in G$, called the \emph{identity}, such that
for all $g \in G, e\cdot g = g$, and (4) for all $g \in G$ there exists an \emph{inverse}
element $g^{-1} \in G$ such that $g^{-1}\cdot g = e$.}
Then, a subset of outcomes is conditionally equally likely, given that the data
fall in a particular orbit of the group: all possible data in the orbit of the
observed data under the action of the group. That makes it possible to
determine the conditional distribution of any test statistic, given the orbit
of the data.  If tests are performed conditionally at level $\alpha$ regardless
of the observed data, the resulting overall test has unconditional level
$\alpha$, by the law of total probability.

%In a permutation test, the distribution of the test statistic under the null
%hypothesis is obtained exactly by computing the test statistic on all possible,
%equally likely relabelings of the observed data, or approximately, by computing
%the test statistic on a random sample of the equally likely relabelings of the
%observed data.

More formally,
%\begin{itemize}
%\item $G$ is closed under $\star$,
%\item $\star$ is associative,
%\item there exist an $e \in G$, called the identity, such that for all $g \in G, e\star g = g$,
%\item for all $g \in G$ there exists an inverse element $g^{-1} \in G$ such that $g^{-1}\star g = e$.
%\end{itemize}}
the \emph{action} of a (finite) group $\mathcal{G} = (G, \cdot)$ on a (finite) set $X$
is a permutation $\pi_g: X \to X$ for each $g \in G$ such that (1) $\pi_e$ is the identity
(i.e., for every $x \in X, \pi_e(x) = x$) and (2) for every $g_1$ and $g_2$ in $G$,
$\pi_{g_1} \circ \pi_{g_2} = \pi_{g_1 \cdot g_2}$.  Following standard practice, I write $\pi_g(x)$ as $gx$.
This is simplified notation for the effect of the permutation associated with $g$ on the
element $x$ and is not meant to suggest that we can multiply elements of $G$ with elements of $X$.
For any $x \in X$, the \emph{orbit} $\mathcal{G}(x)$ of $x$ in $X$ is
the set of elements (or points) of $X$ to which $x$ moves under the group
action of $\mathcal{G}$; that is,
$\mathcal{G}(x) = \Set{gx \in X \given g \in G}$
where $g$ runs over all elements of $G$.   

Permutation testing involves conditional probabilities (i.e., conditional on
the event that the hypothetical generated data lie in the orbit of the observed
data) based on the assumption that the null hypothesis is true.  The group is
determined by considering the underlying symmetries of the observed data under
the null, which are equivariant.  These  conditional probability distributions
allow you to find the probability of any event.  In the following examples, we
will restrict our attention to the probability the test statistic is \emph{as
or more extreme} than observed, under the null conditioned on the observed
data.  Since the conditional distribution is uniform on the orbit of the
original data, the probability of any event is the proportion of possible
outcomes that lie in the event.

\begin{example}[label=exa:cont] Consider the following randomized, controlled
experiment.  You suspect a specific treatment will increase the growth rate of
a certain type of cell.  To test this hypothesis, you clone 100 cells. Now
there are 200 cells composed of 100 pairs of identical clones. For each cloned
pair you randomly assign one to treatment, with probability 1/2, independently
across the 100 pairs.  At the end of the treatment, you measure the growth rate
for all the cells.  The null hypothesis is that this treatment has no effect.
If that is true, then the assignment of a clone to treatment amounts to an
arbitrary label that has nothing to do with the measured response.  So you can
generate new hypothetical datasets from the observed data by exchanging the
treatment and control labels for all the cloned pairs independently.  This
yields a total of $2^{100}$ total datasets (including the observed data and all
the hypothetical datasets that you generated), which are equally likely to have
occurred under the null, conditioning on the observed data.

The standard parametric approach to this problem is the paired $t$-test, since
the cloned cells are presumably more similar to each other than to another
randomly chosen cell (and thus more readily compared).  The paired $t$-test
requires you to assume that the differences between each paired clone are
independently and identically (iid) normally distributed with unknown variance.
The test statistic is the mean of the differences between each cloned pair
divided by the standard error of these differences.  The null hypothesis is
that the iid normally distributed differences have zero mean (i.e., $d_i \iid
\N{0}{\sigma^2}$ where $d_i$ is the difference in growth rate of the
$i^{\text{th}}$ cloned pair and $\sigma^2$ is an unknown constant).  The
alternative is that the mean difference of the underlying population is
non-zero.  Under these additional assumptions, the test statistic is
distributed as a $t$-distribution with $n-1$ degrees of freedom.  This means you can
calculate the test statistic and then read off the p-value from the
$t$-distribution.  If the p-value is below some prespecified critical value
$\alpha$, then you reject the null.  However, this may lead you to reject the
null because the distributions are not normally distributed and not because the
means are different.

Using permutation testing you can use the same test statistic as for
a paired $t$-test, but instead of calibrating the significance against the
$t$-distribution you would use the exact distribution of the test statistic
under the null by computing it on all equally likely possible datasets (i.e.,
the observed data as well as your hypothetical generated data).  However, you
can choose another test statistic if you wish.  For example, if the alternative
hypothesis is that treatment increases response and you are concerned with
extreme outliers in your observed data, you might prefer to use either the
\emph{median difference in response between the treated and the controls} or
the \emph{number of treated clones that have larger responses than their
paired control} as your test statistic.

Regardless of which test statistic you choose for your permutation test, if the
problem size is not too large then you enumerate all equally likely
possibilities under the null given the observed data.  If the problem is too
large to feasibly enumerate, then you use a suitably large, random uniform
sample from the exact distribution just described.  As you increase the number
of samples, you will get increasingly better approximations of the exact
distribution of the test statistic under the null.  The permutation p-value is
then the proportion of the total number of test statistics that are \emph{as}
or \emph{more extreme} than the observed test statistics.  \end{example}

\begin{example} Given $n=10$ observations of two scalars $(x_i, y_i)$ for
$i = 1, 2, \dots, n$, consider the simple linear regression model
$y_i = a + bx_i + \epsilon_i$.  Assume that the $\epsilon_i$'s are
exchangeable.\footnote{A sequence $X_1, X_2, X_3, \dots, X_n$ of random
variables is \emph{exchangeable} if their joint distribution is invariant to
permutations of the indices; that is,
\begin{align*}
p(x_1, \dots, x_n) &= p(x_{\pi(1)}, \dots, x_{\pi(n)})
\end{align*}
for all permutations $\pi$ of $1, 2, \dots, n$.  It is closely related to the
notion of \emph{independent and identically-distributed} random variables.
Independent and identically-distributed random variables are exchangeable.
However, simple random sampling \emph{without} replacement produces an
exchangeable, but not independent, sequence of random variables.}

You are interested in showing that the slope of the population regression line
is non-zero; hence, your null hypothesis is $b = 0$. If $b = 0$, then the model
reduces to $y_i = a + \epsilon_i$ for all $i$.  If this is true, the
$y_i$s are exchangeable since they are just shifted versions of the
exchangeable $\epsilon_i$s.  Thus every permutation of the $y$s has the same
probability regardless of the $x$s.  Hence every pairing
$(x_i, y_j)$ for any fixed $i$ and for $j = 1, 2, \dots, n$ is equally likely.

Using the least squares estimate of the slope normalized by its standard error
as the test statistic, you can find it's exact distribution under the null given
the observed data by computing the test statistic on all possible pairs
formed by permuting the $y$ values among the $x$ values.  From the distribution
of the test statistic under the null conditioned on the observed data, the
p-value is the ratio of the count of the \emph{as extreme} or \emph{more
extreme} test statistics to the total number of such test statistics. For
$n=10$ you would enumerate all $10!$ equally likely pairings and then compute
the exact p-value.  For sufficiently large $n$, enumeration becomes infeasible;
in which case, you would approximate the exact p-value using a uniform random
sample of the equally likely pairings.

A parametric approach to this problem would begin by imposing additional
assumptions on the noise $\epsilon$.  For example, if it is assumed that
$\epsilon$ comes from identical Gaussian distributions with mean 0
independently realized for each observed pair, then the test statistic has a
$t$-distribution with $n-2$ degrees of freedom.  If this additional assumption
holds, then we can read the p-value off a table.  Note that, unlike in the
permutation test, we were only able to calculate the p-value (even with the
additional assumptions) because we happened to be able to derive the
distribution of this specific test statistic.
\end{example}

In summary, the general procedure for computing a p-value using permutation
testing is: (1)~carefully formulate the null hypothesis based on the
experimental design and question of interest, (2)~from the observed data,
generate all equally likely possible datasets under the null, (3)~compute the
test statistic for the observed and generated hypothetical data, and 
(4)~calculate the proportion of data sets with test statistic at least as extreme
as the observed.  Note that this exact procedure works regardless of the
particular test statistic chosen.  If step (2) is too computationally expensive
to perform, then you can approximate the exact p-value by the same procedure as
above except that you replace step (2) with ($2^\prime$)~from the observed
data, generate a sufficiently large uniform random sample of all equally likely
possible datasets under the null. Simulating the distribution via random
sampling will produce arbitrarily good approximate results as you increase your
sample size.

The advantage of the permutation approach is that it produces exact (or---if we
sample from the distribution of the test statistic rather than enumerate it
completely---approximately exact) results with weaker assumptions 
about the generating process than necessary using parametric tests since we do
not need to derive an analytic form for the distribution of our chosen test statistic.
Further, the weaker assumptions necessary for a permutation test can be
verified directly from the randomization of the experimental design.  Thus
permutation testing allows us to focus on the exact question of interest,
rather than a similar question which is amenable to analysis.  Additionally,
the rationale justifying these claims does not require asymptotic theory; this
means we have exact results for small samples.  The disadvantage is the
computational cost; however, with increasing computational resources and
improved algorithms, this limitation is less of a concern.

\section{Confidence sets}

Given $X \sim P_\theta$ for some $\theta \in \Theta$,\footnote{In general, we allow any
arbitrary set $\Theta$ to index the family of distributions. In the following example, we will take $\Theta =\reals$.} a random set
$C(X)$ is a $1 - \alpha$ confidence set for $\theta$ if
\begin{align*}
P_\theta(\theta \in C(X)) &\ge 1 - \alpha
\end{align*}
for all $\theta \in \Theta$.

For every $\theta_0 \in \Theta$, let $A(\theta_0)$ be the acceptance region for
a test at level $\alpha$ of the null hypothesis $H_{\theta_0}: \theta =
\theta_0$ versus the alternative that $\theta_0$ is not the true $\theta$.
Denote the family of all such null hypotheses $\mathcal{H} = \Set{H_{\theta_0}
\given \theta_0 \in \Theta}$.  Thus we have a family of significance-level
$\alpha$ tests $\Set{A(\theta_0) \given \theta_0 \in \Theta}$ such that for
each $H_{\theta_0} \in \mathcal{H}$,
\begin{align*}
P_{\theta_0}(X \notin A(\theta_0)) \le \alpha.
\end{align*}

Define $C(x) \doteq \Set{\theta \in \Theta \given x \in A(\theta)}$.
So by construction $\theta \in C(X)$ exactly when $X \in A(\theta)$, and so
\begin{align*}
P_{\theta}(\theta \in C(X)) = P_{\theta}(X \in A(\theta)) \le 1 - \alpha.
\end{align*}

This result allows us to construct a $1-\alpha$ confidence set dual to
the family of tests with acceptance regions $A(\theta_0)$.\footnote{Conversely,
we can construct families of tests from confidence sets.}  Despite the duality
between tests and confidence sets, computing the permutation confidence set
often requires additional assumptions in order to construct a family of
permutation tests with acceptance regions $A(\theta_0)$ for all $\theta_0 \in
\Theta$.

For instance in Example 1 above, we could form an acceptance region for a level
$\alpha$ permutation test for the null hypothesis that the treatment had no
effect by stipulating that the p-value under the null given the observed data
is greater than $\alpha$.  However, using only the original assumptions, it is
unclear how you would form acceptance regions for a level $\alpha$ permutation
test of any other null hypotheses. 

\begin{example}[continues=exa:cont] To illustrate how to form $1-\alpha$
confidence sets from a family of level $\alpha$ permutation tests, we further
assume that the effect of treatment in the paired clone experiment is explained
by a \emph{shift model}.  The shift model assumes that the treatment effect
adds an unknown constant real number $d$ to the control response of the cell.
Let $(t_i, c_i)$ denote the observed growth rates of the $i^\text{th}$ cloned
pair where $t_i$ corresponds to the treated clone and $c_i$ the control.  Then
the shift model is that for each treated clone the effect of treatment is
equivalent to the hypothetical growth rate it would have had without treatment
$t_i^c$ plus the constant treatment shift $d$ (i.e., $t_i = t_i^c + d$).

Let $H_{d^\prime}$ denote the null hypothesis that $d=d^\prime$ for any
$d^\prime \in \reals$.  Now we can form a permutation test of $H_{d^\prime}$
versus the alternative that $d \neq d^\prime$ in a similar way as we did when
finding the permutation p-value for the null hypothesis that treatment had no
effect. For each fixed value of $d^\prime$, consider the hypothetical
pretreatment cloned pair $(a_i, b_i)$ where $a_i\doteq t_i-d^\prime$ and $b_i
\doteq c_i$. If $H_{d^\prime}$ is true, then all $2^n$ possible allocations of
the treatment shift $d^\prime$ to one of cloned pairs $(a_i, b_i)$ represents a
equally likely hypothetical treatment outcome. From this exact distribution you
can find the permutation p-value for your desired test statistic (e.g., the
median difference between treatment and control). Hence a level $\alpha$ family
of test of $H_{d^\prime}$ versus the alternative can be specified by accepting
the alternative when the permutation p-value (under the null and conditioned on
the data) is less than $\alpha$ and accepting $H_{d^\prime}$ otherwise. 

Denote the family of null hypotheses $\mathcal{H} = \{H_{d^\prime} : d^\prime
\in \reals\}$.  Thus we have a family of level $\alpha$ permutation tests
$\Set{A(d^\prime) \given d^\prime \in \reals}$. Now define $C(x) \doteq \Set{d' \given x \in
A(d')}$.  So by the above argument, $C(X)$ is a $1-\alpha$ confidence set
for the true $d$ dual to the family of permutation tests.
Thus to find the confidence interval for $d$ we use these tests as follows.
Let $d^\prime$ be the estimated treatment effect given by your test statistic.
Form the hypothetical pretreatment cloned pair $(a_i, b_i)$ using your estimate
of $d$.  Then determine the exact p-value of $d^\prime$ under the null
hypothesis $H_{d^\prime}$ given the observed data. If the p-value is greater
than $\alpha$ include it in your confidence interval.  Systematically increase
(decrease) $d^\prime$ until the permutation test rejects to get the upper (lower)
bounds on the confidence interval.

By assuming a shift model we are able to invert the permutation test to form
confidence sets.  However, it should be noted that assuming a constant additive
effect of treatment across all cell pairs may be unwarranted. It is likely
that individual variability among cloned pairs would correspond to variability
in treatment effect.  For instance, if the treatment involves administering a
nutritional supplement, then there is a possibility that the different cloned
pairs will have differing abilities to incorporate it.

%\begin{algorithmic}[1]
%\While{some condition holds}
%\State compute the compact SVD of $R^\top = U_r S_r V^\top_r$
%\State compute $\tilde{A} = LU_r S_r$
%\State compute $\tilde{x}_r = V^\top_r x$
%\If{X=1}
%  \State d
%\Else
%  \State d
%\EndIf
%\EndWhile
%\end{algorithmic}

\end{example}

\section{Python}

Python is a high-level, general purpose programming language, which has become
increasingly popular for scientific computing \cite{millman2011python,
Perez2011}. Unlike some high-level languages used in scientific computing,
Python was not specifically designed for scientific applications.  However, it
quickly attracted interest among scientists and engineers.  Initially, it was
employed primarily as a ``glue'' language to couple together compiled binaries
for scientific applications written in C or Fortran \cite{dubois2007guest}.

As more scientists and engineers began using Python, they started developing
third party libraries to provide additional functionality for scientific and
numeric computing.  In particular, NumPy\footnote{\url{http://numpy.org}},
SciPy\footnote{\url{http://scipy.org}}, and
matplotlib\footnote{\url{http://matplotlib.org}} provide a core foundation on
which other scientific Python packages (such as \texttt{permute}) build 
\cite{oliphant2007python, van2011numpy, hunter2007matplotlib}. NumPy
provides the basic n-dimensional array data structure and a small number of
basic functions (e.g., linear algebra, Fourier transforms) to compute on this data
structure.  SciPy adds additional general routines on top of this core
functionality necessary for scientific computing including basic statistics and
optimization.  Complementing these data structures and algorithms, matplotlib
provides publication quality 2D plotting.  

While it is beyond the scope of this report to explore these packages in more
detail, I note that the pseudo-random number generator (PRNG) provided by NumPy
is the Mersenne Twister.  The Mersenne Twister is an efficient PRNG with a
sufficiently large period for most statistical simulations.\footnote{It is the
default PRNG in R as well.  While sufficient for most statistical simulations,
for other applications such as cryptography it may be insufficient.}  In
addition to providing a high-quality PRNG, NumPy implements Knuth
shuffling---an efficient (and simple) algorithm for uniformly generating
permutations of sequences.

While Python is similar to R in many respects and is widely used in scientific
and numerical computing, it lacks R's extensive support for statistical
applications.  Recently, however, as an increasing number of data scientists
have embraced Python as their primary programming language they have developed
several new Python packages including Pandas, statsmodels, and
scikit-learn.  These new packages have greatly enhanced Python as a language for
statistical computing.  Our intention is to help accelerate this trend by supplying a
high-quality, rigorously tested, and statistically sound package for a large
variety of permutation tests and confidence sets. As the package matures, we
anticipate contributing generic functionality upstream to the packages we
depend on.

\section{Putting it all together}

Our aim is for \texttt{permute} to increase the use of permutation methods
as well as the usefulness of Python for statistical data analysis.
While we wish to see an increased use of permutation methods, our aim is to not
only make these methods easier to apply, but also to help researchers
apply these methods correctly.  To this end, we intend to provide many worked
examples illustrating the thought process and not just the mechanical
application of these methods.  Further our goal is to provide tools to
prototype and implement custom tests such that researchers can easily develop
permutation tests appropriate for their experiments.  As suggested by our
examples, experimental design needs to be carefully considered in designing
permutation tests. We would like to provide the tools that will enable scientists
to conduct tests that correspond to their designs.  Finally, we aim to
provide Python tools to statisticians as well as better statistical tools to
Python users.
