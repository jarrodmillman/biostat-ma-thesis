\chapter{\label{ch:intro}Introduction}

\texttt{permute} is a Python package for permutation tests and confidence
sets.\footnote{\url{http://statlab.github.io/permute}} Philip B. Stark, Kellie
Ottoboni, St\'{e}fan van der Walt, and I developed this package over the last
year with most of the work occurring during the last few months.  In this
report, I briefly explain the purpose of the package (\S~\ref{ch:intro}), our
development practices (\S~\ref{ch:dev}), the current functionality
(\S~\ref{ch:func}), and our short- and long-term plans (\S~\ref{ch:nextsteps}).

\section{Permutation tests}

Permutation tests (sometimes referred to as randomization, re-randomization, or
exact tests) are a nonparametric approach to statistical significance testing.
They were first introduced by R. A. Fisher in 1935 \cite{fisher1935design} and
further developed by E. J. G. Pitman  \cite{pitman1937,
pitman1938significance}.  After the introduction of the bootstrap, the ideas
were extended in the 1980's by J. Romano \cite{romano1988bootstrap,
romano1989bootstrap}.

Permutation tests were developed to test hypotheses for which relabeling the
observed data was justified by 
exchangeability\footnote{A sequence $X_1, X_2, X_3, \dots, X_n$ of random
variables is \emph{exchangeable} if their joint distribution is invariant to
permutations of the indices; that is,
\begin{align*}
p(x_1, \dots, x_n) &= p(x_{\pi(1)}, \dots, x_{\pi(n)})
\end{align*}
for all permutations $\pi$ of $\Set{1, 2, \dots, n}$.  It is closely related to the
notion of \emph{independent and identically-distributed} random variables.
Independent and identically-distributed random variables are exchangeable.
However, simple random sampling \emph{without} replacement produces an
exchangeable, but not independent, sequence of random variables.}
of the observed random variables.  In these situations, the
conditional distribution of the test statistic under the null hypothesis is completely
determined by the fact that all relabelings of the data are equally likely.
That distribution might be calculable in closed form; if not, it can be simulated
with arbitrary accuracy by generating relabelings uniformly at random.
In contrast to approximate parametric methods or asymptotic methods, the accuracy
of the simulation for any finite (re)-sample size is known, and can be made
arbitrarily small at the expense of computing time.

More generally, permutation tests are possible whenever the
null distribution of the data is invariant under the action of some group (see Appendix~\ref{app:def} for background).
%\footnote{A group $\mathcal{G} = (G, \cdot)$ is a tuple
%where $G$ is a set of elements and $\cdot$ is an operation that takes any two elements
%of $G$ and returns another element of $G$ such that (1) $G$ is \emph{closed} under $\cdot$,
%(2) $\cdot$ is \emph{associative}, (3) there exist an $e \in G$, called the \emph{identity}, such that
%for all $g \in G, e\cdot g = g$, and (4) for all $g \in G$ there exists an \emph{inverse}
%element $g^{-1} \in G$ such that $g^{-1}\cdot g = e$.}
Then, a subset of outcomes is conditionally equally likely, given that the data
fall in a particular \emph{orbit} of the group (all potential observations that result from
applying elements of the group to the observed value of the data).
That makes it possible to
determine the conditional distribution of any test statistic, given the orbit
of the data.
Since the conditional distribution is uniform on the orbit of the
original data, the probability of any event is the proportion of possible
outcomes that lie in the event.
If tests are performed conditionally at level $\alpha$ regardless
of the observed data, the resulting overall test has unconditional level
$\alpha$, by the law of total probability.

%More formally,
%\begin{itemize}
%\item $G$ is closed under $\star$,
%\item $\star$ is associative,
%\item there exist an $e \in G$, called the identity, such that for all $g \in G, e\star g = g$,
%\item for all $g \in G$ there exists an inverse element $g^{-1} \in G$ such that $g^{-1}\star g = e$.
%\end{itemize}}
%the \emph{action} of a (finite) group $\mathcal{G} = (G, \cdot)$ on a (finite) set $X$
%is a permutation $\pi_g: X \to X$ for each $g \in G$ such that (1) $\pi_e$ is the identity
%(i.e., for every $x \in X, \pi_e(x) = x$) and (2) for every $g_1$ and $g_2$ in $G$,
%$\pi_{g_1} \circ \pi_{g_2} = \pi_{g_1 \cdot g_2}$.  Following standard practice, I write $\pi_g(x)$ as $gx$.
%This is simplified notation for the effect of the permutation associated with $g$ on the
%element $x$ and is not meant to suggest that we can multiply elements of $G$ with elements of $X$.
%For any $x \in X$, the \emph{orbit} $\mathcal{G}(x)$ of $x$ in $X$ is
%the set of elements (or points) of $X$ to which $x$ moves under the group
%action of $\mathcal{G}$; that is,
%$\mathcal{G}(x) = \Set{gx \in X \given g \in G}$
%where $g$ runs over all elements of $G$.   

%Permutation testing involves conditional probabilities (i.e., conditional on
%the event that the hypothetical generated data lie in the orbit of the observed
%data) based on the assumption that the null hypothesis is true.  The group is
%determined by considering the underlying symmetries of the observed data under
%the null, which are equivariant.  These  conditional probability distributions
%allow you to find the probability of any event.  In the following examples, we
%will restrict our attention to the probability the test statistic is \emph{as
%or more extreme} than observed, under the null conditioned on the observed
%data.  


\begin{example}[label=exa:cont] Consider the following randomized, controlled
experiment.  You suspect a specific treatment will increase the growth rate of
a certain type of cell.  To test this hypothesis, you clone 100 cells. Now
there are 200 cells composed of 100 pairs of identical clones. For each cloned
pair you randomly assign one to treatment, with probability 1/2, independently
across the 100 pairs.  At the end of the treatment, you measure the growth rate
for all the cells.  The null hypothesis is that treatment has no effect.
If that is true, then the assignment of a clone to treatment amounts to an
arbitrary label that has nothing to do with the measured response.  So, given
the responses within each pair (but not the knowledge of which clone in each
pair had which response), it would have been just as likely to observe
the same \emph{numbers} but with flipped labels within each pair.
We could
generate new hypothetical datasets from the observed data by assigning the
treatment and control labels for all the cloned pairs independently.  This
yields a total of $2^{100}$ total datasets (including the observed data and all
the hypothetical datasets that you generated), all equally likely to have
occurred under the null, conditioning on the observed data (but not the labeling).

The standard parametric approach to this problem is the paired $t$-test, since
the cloned cells are presumably more similar to each other than to another
randomly chosen cell (and thus more readily compared).  The paired $t$-test
assumes that, if the null hypothesis is true, the differences in response
between each pair of clones are 
independently and identically (iid) normally distributed with mean zero and unknown variance.
The test statistic is the mean of the differences between each cloned pair
divided by the standard error of these differences.  
Under these assumptions, the test statistic is
distributed as a $t$-distribution with $n-1$ degrees of freedom.  This means you can
calculate the test statistic and then read off the p-value from the
$t$-distribution.  If the p-value is below some prespecified critical value
$\alpha$, then you reject the null.  
If the true generative model for the data is not iid normal, however, the
probability of rejecting the null hypothesis can be quite different from $\alpha$
even if treatment has no effect.

A permutation version of the $t$-test can avoid that vulnerability: one can
use the $t$-statistic as the test statistic, but instead of selecting the critical
value on the basis of Student's $t$-distribution, one uses the
the distribution of the statistic under the permutation distribution.
Of course, other test statistics could be used instead; the test statistic
should be sensitive to the nature of the alternative hypothesis, to ensure
that the test has power against the alternatives the science suggests are
relevant.

Regardless of which test statistic you choose for your permutation test, if the
problem size is not too large then you enumerate all equally likely
possibilities under the null given the observed data.  If the problem is too
large to feasibly enumerate, then you use a suitably large, iid random 
sample from the exact distribution just described, by selecting permutations
uniformly at random and applying the test statistic to those permutations.  As you increase the number
of samples, you will get increasingly better (in probability) approximations of the exact
distribution of the test statistic under the null.  The null conditional probability of any
event can be estimated as the proportion of random permutations for which the event occurs,
and the sampling variability of that estimate can be characterized exactly, for instance,
using binomial tests (since the distribution of the number of times the event occurs
is Binomial with $n$ equal to the number of samples and $p$ the unknown probability to be
estimated). 
\end{example}

\begin{example} Given $n=10$ observations of two scalars $(x_i, y_i)$ for
$i = 1, 2, \dots, n$, consider the simple linear regression model
$y_i = a + bx_i + \epsilon_i$.  Assume that $\{\epsilon_i\}_{i=1}^{10}$ are
exchangeable.

You are interested in testing whether the slope of the population regression line
is non-zero; hence, your null hypothesis is $b = 0$. If $b = 0$, then the model
reduces to $y_i = a + \epsilon_i$ for all $i$.  If this is true, the
$\{y_i\}_{i=1}^{10}$ are exchangeable since they are just shifted versions of the
exchangeable $\{\epsilon_i\}_{i=1}^{10}$.  Thus every permutation of the $\{y_i\}_{i=1}^{10}$ has the same
conditional probability regardless of the $x$s.  Hence every pairing
$(x_i, y_j)$ for any fixed $i$ and for $j = 1, 2, \dots, n$ is equally likely.

Using the least squares estimate of the slope normalized by its standard error
as the test statistic, you can find its exact distribution under the null given
the observed data by computing the test statistic on all possible pairs
formed by permuting the $y$ values, keeping the original order of the $x$ values.  From the distribution
of the test statistic under the null conditioned on the observed data, the
p-value is the ratio of the count of the \emph{as extreme} or \emph{more
extreme} test statistics to the total number of such test statistics. For
$n=10$ you might in principle enumerate all $10!$ equally likely pairings and then compute
the exact p-value.  For sufficiently large $n$, enumeration becomes infeasible;
in which case, you could approximate the exact p-value using a uniform random
sample of the equally likely pairings.

A parametric approach to this problem would begin by imposing additional
assumptions on the noise $\epsilon$.  For example, if we assume
that $\{\epsilon_i\}$ are iid Gaussians with mean zero, 
then the test statistic has a
$t$-distribution with $n-2$ degrees of freedom.  If this additional assumption
holds, then we can read the p-value off a table.  Note that, unlike in the
permutation test, we were only able to calculate the p-value (even with the
additional assumptions) because we happened to be able to derive the
distribution of this specific test statistic.
\end{example}

In summary, the general procedure for computing a p-value using permutation
testing is to formulate the null hypothesis based on the
experimental design and question of interest and then determine whether,
under the null hypothesis, the probability distribution of the data is invariant
under the action of some group. 
If so, then given the orbit of the observed data, every element of that
orbit is conditionally equally likely; with that knowledge,
one can either calculate the null conditional probability distribution
of any test statistic or simulate it to any desired level of accuracy. 

A big advantage of the permutation approach is that it produces (essentially) exact 
tests with weaker assumptions 
about the generating process than parametric tests require.
Those weaker assumptions may follow directly from the 
the experimental design.  Thus
permutation testing allows us to focus on the exact question of interest,
rather than a similar question that is amenable to analysis.  Additionally,
justifying these claims does not require asymptotic theory; 
we get exact results for small samples.  A potential disadvantage is the
computational cost; however, with increasing computational resources and
improved algorithms, this limitation is less of a concern.
Moreover, permutation tests are easier for most people to understand than
parametric tests are.

\section{Confidence sets}

Given $X \sim P_\theta$ for some $\theta \in \Theta$,\footnote{In general, we allow any
arbitrary set $\Theta$ to index the family of distributions. In the following example, we will take $\Theta =\reals$.} a random set
$C(X)$ is a $1 - \alpha$ confidence set for $\theta$ if
\begin{align*}
P_\theta(\theta \in C(X)) &\ge 1 - \alpha
\end{align*}
for all $\theta \in \Theta$.

For every $\theta_0 \in \Theta$, let $A(\theta_0)$ be the acceptance region for
a test at level $\alpha$ of the null hypothesis $H_{\theta_0}: \theta =
\theta_0$ versus the alternative that $\theta_0$ is not the true $\theta$.
Denote the family of all such null hypotheses $\mathcal{H} = \Set{H_{\theta_0}
\given \theta_0 \in \Theta}$.  Thus we have a family of significance-level
$\alpha$ tests $\Set{A(\theta_0) \given \theta_0 \in \Theta}$ such that for
each $H_{\theta_0} \in \mathcal{H}$,
\begin{align*}
P_{\theta_0}(X \notin A(\theta_0)) \le \alpha.
\end{align*}

Define $C(x) \equiv \Set{\theta \in \Theta \given x \in A(\theta)}$.
So by construction $\theta \in C(X)$ exactly when $X \in A(\theta)$, and so
\begin{align*}
P_{\theta}(\theta \in C(X)) = P_{\theta}(X \in A(\theta)) \le 1 - \alpha.
\end{align*}

This result allows us to construct a $1-\alpha$ confidence set dual to
the family of tests with acceptance regions $A(\theta_0)$.\footnote{Conversely,
we can construct families of tests from confidence sets.}  Despite the duality
between tests and confidence sets, computing the permutation confidence set
often requires additional assumptions in order to construct a family of
permutation tests with acceptance regions $A(\theta_0)$ for all $\theta_0 \in
\Theta$.

For instance in Example 1 above, we could form an acceptance region for a level
$\alpha$ permutation test for the null hypothesis that the treatment had no
effect by stipulating that the p-value under the null given the observed data
is greater than $\alpha$.  However, using only the original assumptions, it is
unclear how you would form acceptance regions for a level $\alpha$ permutation
test of any other null hypotheses. 

\begin{example}[continues=exa:cont] To illustrate how to form $1-\alpha$
confidence sets from a family of level $\alpha$ permutation tests, we further
assume that the effect of treatment in the paired clone experiment is explained
by a \emph{shift model}.  The shift model assumes that the treatment
adds an unknown constant real number $d$ to the control response of the cell.
Let $(t_i, c_i)$ denote the observed growth rates of the $i^\text{th}$ cloned
pair where $t_i$ corresponds to the treated clone and $c_i$ the control.  Then
the shift model is that for each treated clone the effect of treatment is
equivalent to the hypothetical growth rate it would have had without treatment
$t_i^c$ plus the constant treatment shift $d$ (i.e., $t_i = t_i^c + d$).

Let $H_{d^\prime}$ denote the null hypothesis that $d=d^\prime$ for any
$d^\prime \in \reals$.  Now we can form a permutation test of $H_{d^\prime}$
versus the alternative that $d \neq d^\prime$ using a procedure analogous to the
one we used to test the null hypothesis that treatment had no
effect. For each fixed value of $d^\prime$, consider the hypothetical
pretreatment cloned pair $(a_i, b_i)$ where $a_i\equiv t_i-d^\prime$ and $b_i
\equiv c_i$. If $H_{d^\prime}$ were true, then the data for the $i$th pair would
just as likely have been $(b_i+d', a_i)$ as $(t_i, c_i)$, and all pairs are
independent.
That lets us find the p-value of the hypothesis $d=d'$, on the assumption that
the effect of treatment is a simple shift, equal for each clone pair.
Inverting the tests to find the range of values of $d'$ for which we would not
reject $d=d'$ gives a confidence interval for $d$.

By assuming a shift model we are able to invert the permutation test to form
confidence sets.  However, it should be noted that assuming a constant additive
effect of treatment across all cell pairs may be unwarranted. It is likely
that individual variability among cloned pairs would correspond to variability
in treatment effect.  For instance, if the treatment involves administering a
nutritional supplement, then there is a possibility that the different cloned
pairs will have differing abilities to metabolize it.

%\begin{algorithmic}[1]
%\While{some condition holds}
%\State compute the compact SVD of $R^\top = U_r S_r V^\top_r$
%\State compute $\tilde{A} = LU_r S_r$
%\State compute $\tilde{x}_r = V^\top_r x$
%\If{X=1}
%  \State d
%\Else
%  \State d
%\EndIf
%\EndWhile
%\end{algorithmic}

\end{example}

\section{Python}

Python is a high-level, general purpose programming language, which has become
increasingly popular for scientific computing \cite{millman2011python,
Perez2011}. Unlike some high-level languages used in scientific computing,
Python was not specifically designed for scientific applications.  However, it
quickly attracted interest among scientists and engineers.  Initially, it was
employed primarily as a ``glue'' language to couple together compiled binaries
for scientific applications written in C or Fortran \cite{dubois2007guest}.

As more scientists and engineers began using Python, they started developing
third party libraries to provide additional functionality for scientific and
numeric computing.  In particular, NumPy\footnote{\url{http://numpy.org}},
SciPy\footnote{\url{http://scipy.org}}, and
matplotlib\footnote{\url{http://matplotlib.org}} provide a core foundation on
which other scientific Python packages (such as \texttt{permute}) build 
\cite{oliphant2007python, van2011numpy, hunter2007matplotlib}. NumPy
provides the basic n-dimensional array data structure and a small number of
basic functions (e.g., linear algebra, Fourier transforms) to compute on this data
structure.  SciPy adds additional general routines on top of this core
functionality necessary for scientific computing including basic statistics and
optimization.  Complementing these data structures and algorithms, matplotlib
provides publication quality 2D plotting.  

While it is beyond the scope of this report to explore these packages in more
detail, I note that the pseudo-random number generator (PRNG) provided by NumPy
is the Mersenne Twister.  The Mersenne Twister is an efficient PRNG with a
sufficiently large period for most statistical simulations.\footnote{It is the
default PRNG in R as well.  While sufficient for most statistical simulations,
for other applications such as cryptography it may be insufficient.}  In
addition to providing a high-quality PRNG, NumPy implements Knuth
shuffling---an efficient (and simple) algorithm for uniformly generating
permutations of sequences.

While Python is similar to R in many respects and is widely used in scientific
and numerical computing, it lacks R's extensive support for statistical
applications.  Recently, however, as an increasing number of data scientists
have embraced Python as their primary programming language they have developed
several new Python packages including Pandas, statsmodels, and
scikit-learn.  These new packages have greatly enhanced Python as a language for
statistical computing.  Our intention is to help accelerate this trend by supplying a
high-quality, rigorously tested, and statistically sound package for a large
variety of permutation tests and confidence sets. As the package matures, we
anticipate contributing generic functionality upstream to the packages we
depend on.

\section{Putting it all together}

Our aim is for \texttt{permute} to increase the use of permutation methods
as well as the usefulness of Python for statistical data analysis.
While we wish to see an increased use of permutation methods, our aim is to not
only make these methods easier to apply, but also to help researchers
apply these methods correctly.  To this end, we intend to provide many worked
examples illustrating the thought process and not just the mechanical
application of these methods.  Further our goal is to provide tools to
prototype and implement custom tests such that researchers can easily develop
permutation tests appropriate for their experiments.  As suggested by our
examples, experimental design needs to be carefully considered in designing
permutation tests. We would like to provide the tools that will enable scientists
to conduct tests that correspond to their designs.  Finally, we aim to
provide Python tools to statisticians as well as better statistical tools to
Python users.
